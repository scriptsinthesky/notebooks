{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "---\n",
    "- Basis Function: \n",
    "- Bayes' theorem: 𝑝(𝑌|𝑋)=𝑝(𝑋|𝑌)𝑝(𝑌)𝑝(𝑋)\n",
    "- Bayesian: Approach asserts that there is some prior knowledge about the distribution of the data and model parameters. This prior knowledge alone could be used to make predictions about the future observation - however poor they may be. Once data is observed, it is used to *modulate* the prior assumptions. \n",
    "- Binary classifier:Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule.  \n",
    "---\n",
    "- Categorization - algorithms that predict discrete outputs\n",
    "- Compound distribution: Let 𝑡 be a RV with distribution 𝐹 paramaterized by 𝐰 and let 𝐰 be a RV distributed by 𝐺 parameterized by 𝐭, then the compound distribution 𝐻 parameterized by 𝐭 for the random variable 𝑡 is defined by: 𝑝𝐻(𝑡|𝐭)=∫𝐰𝑃𝐹(𝑡|𝐰)𝑃𝐺(𝐰|𝐭)𝑑𝐰\n",
    "- Conditional probability: 𝑝𝑋(𝑋|𝑌) , is the probability distribution of X given that Y has taken some specific value and 𝑝𝑋(𝑥) is the *marginal probability*\n",
    "- Cumulative Distribution Function:  The cumulative distribution function (c.d.f.) of a discrete random variable X is the function F(t) which tells you the probability that X is less than or equal to t. So if X has p.d.f. P(X = x), we have:\n",
    "    F(t) = P(X £ t) = SP(X = x).\n",
    "    In other words, for each value that X can be which is less than or equal to t, work out the probability that X is that value and add up all such results.\n",
    "---\n",
    "- Discrete random variable: a variable which can only take a countable number of values. In this example, the number of heads can only take 4 values (0, 1, 2, 3) and so the variable is discrete. The variable is said to be random if the sum of the probabilities is one.  \n",
    "---\n",
    "- Entropy: the amount of information disorder or the amount of randomness in the data.  \n",
    "---\n",
    "- Frenquentist: approach tends to ignore any such prior knowledge - relying more heavily, or even completely, on the observed data. Both approaches, of course, have adavantages and disadvantages. \n",
    "---\n",
    "- Information gain: the information that can increase the level of certainty after splitting. It is the entropy of a tree before the split minus the weighted entropy after the split by an attribute. We can think of information gain and entropy as opposites. As entropy or the amount of randomness decreases, the information gain or amount of certainty increases and vice versa. \n",
    "---\n",
    "- Joint probability: The probability that 𝑋 and 𝑌 together take some particular value (𝑥,𝑦).\n",
    "---\n",
    "- Learning Theory : study of how and why (mathematically) a learning algorithm works\n",
    "- Likelihood function:\n",
    "---\n",
    "- Machine learning: field of study that gives computers the ability to learn without being explicitly programmed : Arthur Samuel 1959.  A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T as measured by P, improves with E : Tom Mitchell 1998\n",
    "- Marginal probability: The probability distribution of 𝑋 as described above, but when considered as part of a larger set of RVs, as in (𝑋,𝑌) in this case, the probability distribution of an isolated RV is referred to as the marginal probability\n",
    "- Maximum likelihood: see frequentist\n",
    "---\n",
    "- Predictive distribution:\n",
    "- Posterior probability:\n",
    "- Probability density function: The probability density function (p.d.f.) of X (or probability mass function) is a function which allocates probabilities. Put simply, it is a function which tells you the probability of certain events occurring. The usual notation that is used is P(X = x) = something. The random variable (r.v.) X is the event that we are considering. So in the above example, X represents the number of heads that we throw. So P(X = 0) means \"the probability that no heads are thrown\". Here, P(X = 0) = 1/8 (the probability that we throw no heads is 1/8 ).  \n",
    "- Probability distribution function: a table of values showing the probabilities of various outcomes of an experiment.  For example, if a coin is tossed three times, the number of heads obtained can be 0, 1, 2 or 3.  \n",
    "---\n",
    "- Regression - algorithms that predict continuous outputs\n",
    "- Reinforcement Learning : problems where a sequence of decisions are made as opposed to a single decision (or prediction)\n",
    "---\n",
    "- Supervised Learning : training data includes the “correct” answer\n",
    "---\n",
    "- Unsupervised Learning : problems where the algorithm is given a data set without any “right answers”. The objective is to find some underlying structure in the data, e.g. clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
